{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Spark Orielly Book Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= SparkContext(appName=\"schema\", master=\"local[*]\").getOrCreate()\n",
    "ss= SparkSession.builder.appName(\"example_schema\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Schema** <br/>\n",
    "There are 2 ways to define a schema. We will use simplest one which is employing the DDL(Data Defination Language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema= \"name STRING, team STRING, assist INT\" # we defined the schema for dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= [[\"gedson\", \"galatasaray\", 12], [\"pelkas\", \"fenerbahce\", 7], [\"josef\", \"besiktas\", 8]]\n",
    "df= ss.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning With MLib\n",
    "spark.mllib is the original machine learning API, based on the RDD API (which has been in maintenance mode since Spark 2.0), while spark.ml is the newer API, based on Data‚Äê Frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDf= ss.read.csv(\"airbnb.csv\", header=True, inferSchema=False)\n",
    "airbnbDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+--------------+-----------------+-----------+-----------------+\n",
      "|latitude| longitude|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|\n",
      "+--------+----------+-----+--------------+-----------------+-----------+-----------------+\n",
      "|37.77028|-122.43317|  150|             2|              277| 2021-04-05|             1.94|\n",
      "|37.74474|-122.42089|  195|            30|              111| 2017-08-06|             0.76|\n",
      "|37.76555|-122.45213|   56|            32|               19| 2020-03-06|             0.13|\n",
      "|37.76555|-122.45213|   56|            32|                8| 2018-09-12|             0.10|\n",
      "|37.77564|-122.43642|  795|             7|               28| 2019-06-28|             0.20|\n",
      "+--------+----------+-----+--------------+-----------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= airbnbDf.select(\"latitude\", \"longitude\", \"price\", \"minimum_nights\" ,\n",
    "                \"number_of_reviews\" ,\"last_review\", \"reviews_per_month\")\n",
    "df.show(5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6689"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(\"last_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df= df.withColumn(i, df[i].cast(IntegerType()))  #changing the column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: integer (nullable = true)\n",
      " |-- longitude: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: integer (nullable = true)\n",
      " |-- reviews_per_month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+--------------+-----------------+-----------------+\n",
      "|latitude|longitude|price|minimum_nights|number_of_reviews|reviews_per_month|\n",
      "+--------+---------+-----+--------------+-----------------+-----------------+\n",
      "|      37|     -122|  150|             2|              277|                1|\n",
      "|      37|     -122|  195|            30|              111|                0|\n",
      "|      37|     -122|   56|            32|               19|                0|\n",
      "|      37|     -122|   56|            32|                8|                0|\n",
      "|      37|     -122|  795|             7|               28|                0|\n",
      "+--------+---------+-----+--------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= df.na.drop(\"any\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4048 rows in train set.  \n",
      "There are 951 rows in test set.\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test= df.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {df_train.count()} rows in train set.  \\nThere are {df_test.count()} rows in test set.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression (like many other algorithms in Spark) requires that all the input features are contained within a single vector in your DataFrame. Thus, we need to ***transform*** our data. <br/>\n",
    "For putting all the features into single vector we will use the **VectorAssembler transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler= VectorAssembler(inputCols=[\"number_of_reviews\"], outputCol=\"features\")\n",
    "vecTrainDf= vecAssembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+--------------+-----------------+-----------------+--------+\n",
      "|latitude|longitude|price|minimum_nights|number_of_reviews|reviews_per_month|features|\n",
      "+--------+---------+-----+--------------+-----------------+-----------------+--------+\n",
      "|      37|     -122|   10|            30|                1|                0|   [1.0]|\n",
      "|      37|     -122|   25|            30|                2|                0|   [2.0]|\n",
      "|      37|     -122|   25|            30|                4|                0|   [4.0]|\n",
      "|      37|     -122|   25|            30|                4|                0|   [4.0]|\n",
      "|      37|     -122|   25|            30|                5|                0|   [5.0]|\n",
      "+--------+---------+-----+--------------+-----------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecTrainDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel= lr.fit(vecTrainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting the parameters** <br/>\n",
    "**Note:** round function filters the decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression is: \n",
      "price= -0.29*number_of_reviews + 218.67\n"
     ]
    }
   ],
   "source": [
    "c= round(lrModel.coefficients[0], 2)\n",
    "i= round(lrModel.intercept, 2)\n",
    "\n",
    "print(\"The formula for the linear regression is: \\nprice= {}*number_of_reviews + {}\".format(c,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel= pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying it to test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+-----+------------------+\n",
      "|number_of_reviews|features|price|        prediction|\n",
      "+-----------------+--------+-----+------------------+\n",
      "|                2|   [2.0]|   10|218.09041633638142|\n",
      "|               19|  [19.0]|   10|213.19946428258487|\n",
      "|                8|   [8.0]|   23|216.36419796445324|\n",
      "|                2|   [2.0]|   27|218.09041633638142|\n",
      "|                4|   [4.0]|   27|217.51501021240537|\n",
      "+-----------------+--------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDf= pipelineModel.transform(df_test)\n",
    "predDf.select(\"number_of_reviews\", \"features\", \"price\" ,\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparseVector \n",
    "SparseVectors work: <br/>\n",
    "    DenseVector(0, 0, 0, 7, 0, 2, 0, 0, 0, 0) <br/>\n",
    "    SparseVector(10, [3, 5], [7, 2]) <br/>\n",
    "The DenseVector in this example contains 10 values, all but 2 of which are 0. To cre‚Äê ate a SparseVector, we need to keep track of the size of the vector, the indices of the nonzero elements, and the corresponding values at those indices. In this example the size of the vector is 10, there are two nonzero values at indices 3 and 5, and the corre‚Äê sponding values at those indices are 7 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 value of our model: 0.006958209446088159\n"
     ]
    }
   ],
   "source": [
    "regEvaluator= RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                 labelCol=\"price\",\n",
    "                                 metricName=\"r2\")\n",
    "model_rmse= regEvaluator.evaluate(predDf)\n",
    "print(\"R2 value of our model: {}\".format(model_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving and Loading Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath= \"/Users/ahmetemintek/Desktop/new_pyspark/lr_pipeline_model\"\n",
    "pipelineModel.write().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel= PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-nlp",
   "language": "python",
   "name": "project-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
